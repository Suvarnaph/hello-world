{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageLoader_V4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suvarnaph/hello-world/blob/master/ImageLoader_V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "H47bVoA8a3_r",
        "colab_type": "code",
        "outputId": "efc62fc7-9e9a-4d9d-deb9-ef62ff7334c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Import statements\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "from pathlib import Path\n",
        "import re\n",
        "import os\n",
        "from os import getcwd\n",
        "from os.path import exists, isdir, isfile, join\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-LtaYnrB8NXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Image_Loader:\n",
        "  \n",
        "    def __init__(self, root_path, source_dir_name, size, img_read_format, img_type = ('*.jpg', '*.png')):\n",
        "        self.project_path = root_path #root directory path of Mura-v1.1 dataset\n",
        "        self.root = Path(self.project_path)\n",
        "        self.source_directory = Path.joinpath(self.root, source_dir_name) #path from which image has to be read - Ex: train/test complete path\n",
        "        self.scaled_size = size #Dimension to which image has to be rescaled\n",
        "        self.formatRead = img_read_format #Format in which image has to be read : cv2.IMREAD_GRAYSCALE is by default used in this class by image_read_resize function#Future use #TODO\n",
        "        self.img_format_type = img_type #image's actual format, currently .jpg, .png are default\n",
        "        self.x_list=[]\n",
        "        self.y_list=[]\n",
        "        \n",
        "    #Labelling Images\n",
        "    #study1_positive and study1_negative are the only expected ones that can be processed as per MURA-v1.1 dataset\n",
        "    def label_img(self,img):\n",
        "      word_label = os.path.basename(os.path.dirname(img))\n",
        "      print(word_label)\n",
        "      if word_label == 'study1_positive': return 1\n",
        "      elif word_label == 'study1_negative': return 0\n",
        "      \n",
        "      \n",
        "      \n",
        "    #Image_processing method\n",
        "    def image_read_resize(self, file, scaled_size, formatRead = cv2.IMREAD_GRAYSCALE):\n",
        "      #Reading image file in Grayscale mode, channel information removed\n",
        "      #Datagenerator usage TODO\n",
        "      formatRead = cv2.IMREAD_GRAYSCALE #set as default, also hardcoded as it's not possible to convert to numpy array without this#TODO\n",
        "      OrigImage = cv2.imread(file,formatRead)\n",
        "      #resizing image\n",
        "      width = scaled_size\n",
        "      height = scaled_size\n",
        "      dim = (width, height)\n",
        "      resizedImg = cv2.resize(OrigImage, dim)\n",
        "      return resizedImg\n",
        "    \n",
        "    \n",
        "    #Convert Image to list and numpy array\n",
        "    def image_to_list_numpy(self):\n",
        "      print(self.source_directory)\n",
        "      for study_category in self.source_directory.iterdir():\n",
        "        if(os.path.isdir(study_category)):#Only check for directories\n",
        "          print(study_category)\n",
        "          patient_list = sum(1 for _ in study_category.iterdir()) # converting number of patients to list\n",
        "          print(patient_list)\n",
        "          for patient_id in study_category.iterdir():\n",
        "            print(patient_id)\n",
        "            for study_type in patient_id.iterdir():\n",
        "              print(study_type)\n",
        "              #img_format_type = ('*.jpg', '*.png') #TODO: Type of files this expects, need to add exceptions, atleast one general exception, log file,NotADirectoryError\n",
        "              for files in self.img_format_type:\n",
        "                filelist = list(study_type.glob('**/'+ files))\n",
        "              print(filelist)\n",
        "              print(len(filelist))\n",
        "              for fname in filelist:\n",
        "                class_label = self.label_img(fname) #Calling label_img function to get the actual label of the image\n",
        "                print(class_label)\n",
        "                for file in glob.glob(str(fname)):\n",
        "                  print(file)\n",
        "                  resizedImg = self.image_read_resize(file,self.scaled_size) #TODO: Read the image and resize it as per dimensions, maybe multiple uses of scaled_size param can be avoided as its part of class\n",
        "                  self.x_list.append(resizedImg)\n",
        "                  self.y_list.append(class_label)\n",
        "                  x_npy = np.array(self.x_list)\n",
        "                  y_npy = np.array(self.y_list)\n",
        "      np.save((Path.joinpath(self.root,self.source_dir_name,\"Images_array\")), x_npy)\n",
        "      np.save((Path.joinpath(self.root,source_dir_name,\"Labels_array\")), y_npy)\n",
        "      return self.x_list, self.y_list, x_npy, y_npy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dX3FxqH01JhJ",
        "colab_type": "code",
        "outputId": "a236f091-b0ed-4a5b-cd0d-9fb83316cb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_npy.shape, y_train_npy.shape #Number of images stored and its shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6, 224, 224), (6,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "GdPuwys_RxJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Invoking / initializing class\n",
        "objImg = Image_Loader(\"/content/drive/My Drive/AIML/Capstone/MURA-v1.1/\",\"train\",224,cv2.IMREAD_GRAYSCALE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eGmawXowUnwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "5c3ea6f4-c22a-416a-c1d9-fb0b83746217"
      },
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_train_npy, y_train_npy  = objImg.image_to_list_numpy()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND\n",
            "2\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive\n",
            "[PosixPath('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive/image1.png'), PosixPath('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive/image2.png'), PosixPath('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive/image3.png')]\n",
            "3\n",
            "study1_positive\n",
            "1\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive/image1.png\n",
            "study1_positive\n",
            "1\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive/image2.png\n",
            "study1_positive\n",
            "1\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00008/study1_positive/image3.png\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative\n",
            "[PosixPath('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative/image3.png'), PosixPath('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative/image2.png'), PosixPath('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative/image1.png')]\n",
            "3\n",
            "study1_negative\n",
            "0\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative/image3.png\n",
            "study1_negative\n",
            "0\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative/image2.png\n",
            "study1_negative\n",
            "0\n",
            "/content/drive/My Drive/AIML/Capstone/MURA-v1.1/train/XR_HAND/patient00050/study1_negative/image1.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xVt6rgkgXnQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Images = np.load('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/Images_array.npy')\n",
        "labels = np.load('/content/drive/My Drive/AIML/Capstone/MURA-v1.1/Labels_array.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dd_vOthVYN12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1347881-e395-4e0e-b808-5007b5568a1e"
      },
      "cell_type": "code",
      "source": [
        "Images.shape, labels.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6, 224, 224), (6,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}